auto jdbc partitioning
further jdbc predicate pushdown
 - composite pk scrolling :|
jdbc aggregate pushdown
yarn
 - http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/WritingYarnApplications.html
(python) ffi
event sources
random access stores
first layer caching
vitess shit
nashorn stored procs
salesforce connector?
hbase plz

local fs / hive configurator

https://www.simba.com/data-access/apache-drill-data-sources-file-types
SELECT name1[‘nestedname1’][‘nestednestedname1’] FROM <schema>.<filename>.json

tachyon temp spilling
sorted [leveldb] join storage

schema tracker interface
carbide's full ddl parser impl
service impl

yaml config + velocity templating

BLOOM FILTER PREDICATE PUSHDOWN
 - naw temp tables - engine=memory, index(), pk
 - no, TEMP TABLES :D engine=memory, pk clustered + indices

move kafka decoder to base?
 - no, allow cross-plugin dep, recursive plugin loading
  - dag?

necessary for chunked operation:
bounded in-clause pushdown
occasional join pushdown
view inlining

connector capability enumeration
 - crud
 - complex tuple domains
  - joins, bitwise
  - nested disjuncts?

whitebox / blackbox plugins?


rbr tailing
integrate carbide ddl parser to directly modify metadata (WOW)


nested types? subtables? disjoint? untyped?


collection aggregates (windows? subselects? typed?)


presto decoder = generic connector wrapper, given a bytes col or single col
 - support for disjoints ala dot-separated column names
 - col regexes? select r(x.*)?
 - schema validation
 - + ffi = http plugin?

wrmsr-packaging? bootstrap?
 - full shading
 - JAVA_HOME

jython
 - a fucking sqlalchemy connector why not

mvel
java
jsr223 artifact resolver?
bulk funcs

http://qubole-eng.quora.com/Caching-in-Presto


** make wrapper hierarchical poms like hadoop does, one per shaded subdep

https://github.com/apache/hadoop-common/tree/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell

https://github.com/shyiko/mysql-binlog-connector-java
https://github.com/addthis/stream-lib

http://blog.sonatype.com/2008/04/how-to-share-resources-across-projects-in-maven/

<dependency>
    <groupId>org.rocksdb</groupId>
    <artifactId>rocksdbjni</artifactId>
    <version>3.10.1</version>
</dependency>

REDSHIFT

eek need on-board views

https://github.com/rcongiu/Hive-JSON-Serde
https://github.com/FasterXML/jackson-dataformat-avro
https://github.com/FasterXML/jackson-datatype-hppc
https://github.com/FasterXML/jackson-dataformat-yaml

https://github.com/facebook/presto/pull/2896

https://cwiki.apache.org/confluence/display/Hive/Tutorial
https://cwiki.apache.org/confluence/display/Hive/Home


* dumb flat json file output
 - complicated by clustering, can just dump to dirs with uuid names as write-only


prepared statements / plan caching



constants?
 - can impl cheap via fns that just return a #, can transform to literals via MH lookup or iface introspection or something

https://github.com/wrmsr/presto-streaming omfg


bitwise operators

hierarchical config - can just fucking redundantly serialize to fit into map<str,str>'s



http://jyni.org/

streams + sqs + gearman

https://github.com/johnewart/gearman-java




lucene analysis scalars


https://github.com/elastic/elasticsearch-hadoop
https://github.com/elastic/stream2es
https://github.com/elastic/elasticsearch-aws


es bulk by length
https://github.com/elastic/elasticsearch-hadoop/tree/master/mr/src/main/java/org/elasticsearch/hadoop
https://github.com/elastic/elasticsearch-hadoop/tree/master/hive/src/main/java/org/elasticsearch/hadoop/hive
https://github.com/elastic/elasticsearch-hadoop/tree/master/yarn



https://github.com/searchbox-io/Jest/tree/master/jest


https://developer.salesforce.com/page/Streaming_API
http://www.salesforce.com/us/developer/docs/api_rest/index_Left.htm#StartTopic=Content/quickstart.htm


parameterized views [indexer generators]



https://mariadb.com/kb/en/mariadb/optimization-and-tuning/
http://phoenix.apache.org/





https://github.com/facebook/presto/pull/2896

https://commons.apache.org/proper/commons-configuration/userguide/howto_utilities.html




https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-Built-inTable-GeneratingFunctions(UDTF)


wrapper will dedupe unshaded jars, write nested classloader
- http://one-jar.sourceforge.net/

rsync powered jarsync bash/py script pls


decoder predicate pushdown to capable stores lols


split on scalar fn

00000000: 2321 2f62 696e 2f73 680a 0a65 7865 6320  #!/bin/sh..exec
00000010: 6a61 7661 202d 586d 7831 4720 2d6a 6172  java -Xmx1G -jar
00000020: 2022 2430 2220 2224 4022 0a0a 504b 0304   "$0" "$@"..PK..



    @Override
    public CompletableFuture<List<ConnectorSplit>> getNextBatch(int maxSize)
    {
        return targets.getNextBatch(maxSize).thenApply(l -> l.stream().map(this::split).collect(Collectors.toList()));
    }


** CHUNK JDBC RETRIEVAL
multi-queries? (dependent partitions)


custom mysql / postgres jdbc? wrmsr-mysql / wrmsr/-postgres?
 - for more efficient multi-sessioning?
  - ideally keep jdbc agnostic

json extraction to unnest

scalar jackson serdes, ideally strongly typeable


src / sink for line files, one text column
 - ffi page sink, post directly to somewhere?

scalar calc parallelization (for cpu-heavy ffi's)
 - special case of batch scalar execution?


http://dev.mysql.com/doc/connector-j/en/connector-j-usagenotes-j2ee-concepts-managing-load-balanced-connections.html

https://github.com/facebook/mysql-5.6/commit/f8e361952612d00979f7cf744f487e48b15cb5a6#diff-1b7266575f084a759d5bee343efe91d0
http://www.oracle.com/technetwork/articles/database-performance/geist-parallel-execution-1-1872400.html


flat:
 - \n, \0, fixedwidth
 - filename regex, string formatted filename

TODO:
 - decoder meta vs unnested extractors :| perf?
 - layered connectors of same schemas as fallbacks (memcache -> mysql)
 - attempt oracle jdbc driver loading
  - fuck that its not in central lmfao eat shit
 - indexer & partitioner metas

ffi to arbitrary jar / classname / methodname
 - + src / sinks


reactor
yarn
 - mesos?

um, shit: struct / union datatypes - only need to be write-only, can fallback to json
 - ... RowParametricType?

select struct('name', name', 'date', date) from ...
 - just make user defined structs :|

view struct inference / auto gen
 - session-specific typesystems?


spark-style stupid jdbc baked query sources
spark-style cache wrapper (for like json files)


redshift:
 - http://docs.aws.amazon.com/redshift/latest/dg/c_redshift_system_overview.html
 - will DEF need (math) agg pushdown lols
 - http://docs.aws.amazon.com/redshift/latest/mgmt/configuring-connections.html#connecting-drivers
 - http://docs.aws.amazon.com/redshift/latest/mgmt/configure-jdbc-connection.html wtf also not in fucking central jfc
  - at least you dont have to fucking sign in
  https://s3.amazonaws.com/redshift-downloads/drivers/RedshiftJDBC41-1.1.1.0001.jar

https://github.com/liquibase/liquibase/tree/master/liquibase-core/src/main/java/liquibase

https://orainternals.files.wordpress.com/2008/07/riyaj_redo_internals_and_tuning_by_redo_reduction_doc.pdf
http://docs.oracle.com/cd/B28359_01/server.111/b28322/gen_rep.htm#STREP011

union connector
 - step 1 just so you dont have to keep typing the fucking catalog.schema. prefix
 - step 2 optinoally combine tables in both if they have same schema
  - naw thats just a fucking union view


hardcoded:
 - views
  - types
 - tables



public class CachingConnectorMetadata
    implements ConnectorMetadata

temporary connector equiv to hardcoded - views + tables
 - naw just an h2 factory that auto-creates a temp schema

generate views upon request - is this macros?



cfg'd in-mem | master thrift port'd hive metastore

autoexec's


http://stackoverflow.com/questions/10929369/how-to-execute-multiple-sql-statements-from-java

try to get a hierarchical subconfig, then try to get a str

memcache is kV only, see what Cass does

dense key ranges only

unpickling?

handlersocket + all that intermediate agg shit

https://github.com/dropbox/PyHive

user defined types via ROW types (w/ named fields)
 struct_extract(field_name, obj) -> val
 auto-gen structs for services based on schemas
 https://github.com/swagger-api/swagger-codegen
 https://github.com/swagger-api/swagger-spec/blob/master/versions/2.0.md

cfg:
 properties
 json
 xml
 yaml

decode:
 raw
 struct
 json
 csv
 xml
 yaml
 avro
 csv/tsv/piped
 cbor
 smile
 pickle

lines
datetimes
deep

http://basho.com/why-vector-clocks-are-hard/
http://basho.com/why-vector-clocks-are-easy/
http://phoenix.apache.org/update_statistics.html
http://en.wikipedia.org/wiki/Behavior_Trees
http://www.cs.man.ac.uk/~norm/papers/surveys.pdf

http://game.itu.dk/cig2010/proceedings/papers/cig10_015_075.pdf


exception handling for bad data :|
 - shit that wont decompress, bad json, etc


            ArchiveInputStream input = new ArchiveStreamFactory()
                    .createArchiveInputStream(originalInput);


record-level connector cache for configs in sqlite and shit
 - convert to scalar for hash lookups?
  - .... just cache the hash join guts?

CAN REPRESENT TYPE-LEVEL CONSTS AS DATALESS TYPES
 compressed_varbinary<bzip>
 could possibly represent gzipped file of json lines as a type, unnest for rows
 jackson<json> -> some_struct
 peanos lols



need eager wrapper on lazy defaults for compression streams

https://prestodb.io/docs/current/connector/kafka-tutorial.html
https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-core-types.html



https://github.com/FasterXML/jackson-module-swagger
 - needs fork + update

set type?
 - just a Map<K, Void>
  - ... so Void type
   - subclassable for typelevel constants


<dependency>
    <groupId>com.jcraft</groupId>
    <artifactId>jsch</artifactId>
    <version>0.1.53</version>
</dependency>

<dependency>
    <groupId>net.razorvine</groupId>
    <artifactId>pyrolite</artifactId>
    <version>4.6</version>
</dependency>



https://github.com/RGBz/aws-s3-class-loader
https://github.com/sampov2/onejar-maven-plugin


<dependency>
    <groupId>org.rocksdb</groupId>
    <artifactId>rocksdbjni</artifactId>
    <version>3.9.1</version>
</dependency>


    // setFetchSize(Integer.MIN_VALUE) is a mysql driver specific way to force streaming results,
    // rather than pulling entire resultset into memory.
    // see http://dev.mysql.com/doc/refman/5.0/en/connector-j-reference-implementation-notes.html
    if (conn.getMetaData.getURL.matches("jdbc:mysql:.*")) {
      stmt.setFetchSize(Integer.MIN_VALUE)
      logInfo("statement fetch size set to: " + stmt.getFetchSize + " to force MySQL streaming ")
    }


types expose funcs w same name that return new instances
execute hardcoded jdbc connectors with where 0=1 if cols not specified
multijvm support to bypass heap size limits



***
just implement a flat raptor StorageEngine, wrap with encoder


http://stackoverflow.com/questions/15524995/adding-another-projects-jar-as-a-resource-using-maven


immediate priorities: codecs, raw raptor storage, spilling, join ordering, bitwise scalar funcs (or, and, xor)


http://www.adellera.it/blog/category/materialized-views/
http://www.adellera.it/blog/2013/04/22/fast-refresh-of-outer-join-only-materialized-views-algorithm-part-1/


HLists? HNode + HNil?

gson?
 http://www.doublecloud.org/2015/03/gson-vs-jackson-which-to-use-for-json-in-java/


https://github.com/facebook/presto/pull/3016 :D
https://github.com/facebook/presto/pull/1937
https://github.com/facebook/presto/pull/3021


<dependency>
    <groupId>redis.clients</groupId>
    <artifactId>jedis</artifactId>
    <version>2.7.2</version>
</dependency>

<dependency>
    <groupId>net.spy</groupId>
    <artifactId>spymemcached</artifactId>
    <version>2.12.0</version>
</dependency>

<dependency>
    <groupId>org.ow2.sat4j</groupId>
    <artifactId>org.ow2.sat4j.core</artifactId>
    <version>2.3.5</version>
</dependency>


https://github.com/yesmeck/jquery-jsonview


com.facebook.presto.operator.PagesIndex <--- DISK SPILL


** NEED TO PUT FAT ROWS LAST. (review text)


on rows:
 - pack fixed
 - pack many into blocks of N - compressssssion
  - just a special case of array/map?
   - row_array? :p

http://www.datastax.com/dev/blog/advanced-time-series-with-cassandra
http://www.tpc.org/tpc_documents_current_versions/pdf/tpch2.17.1.pdf
riak crdt's
https://github.com/FasterXML/jackson-module-swagger
https://github.com/swagger-api/swagger-core/blob/master/pom.xml
http://mesos.apache.org/api/latest/java/
http://docs.oracle.com/cd/B19306_01/server.102/b14220/transact.htm
http://docs.oracle.com/cd/B28359_01/server.111/b28310/onlineredo001.htm#ADMIN11302
http://ss64.com/ora/syntax-redo.html
http://www.pgcon.org/2012/schedule/attachments/258_212_Internals%20Of%20PostgreSQL%20Wal.pdf
https://hackage.haskell.org/package/HList
https://wiki.haskell.org/Heterogenous_collections
http://okmij.org/ftp/Haskell/HList-ext.pdf
http://www.csee.umbc.edu/courses/undergraduate/202/spring07/Lectures/ChangSynopses/modules/m25-hlist/HList.cpp
https://en.wikipedia.org/wiki/Java_bytecode_instruction_listings
https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-4.html
https://www.elastic.co/blog/es-hadoop-2-1-rc1-released

user defined type aliases? parser type literal limitation bypass?

http://mesos.apache.org/api/latest/java/org/apache/mesos/Scheduler.html
https://github.com/apache/mesos/blob/master/src/examples/java/TestFramework.java


find java 8 shebang, exec with minimal heap, spawn child w flock and die

create type atom_state as enum (
	'committing',
	'committed',
	'compensating',
	'compensated'
);

create sequence atom_id;

create table atom (
	id int unique not null,

	root_id int not null references atom (id) on delete restrict,
	parent_id int references atom (id) on delete restrict,
	prev_sibling_id int references atom (id) on delete restrict,
	active_child_id int references atom (id) on delete restrict,

	primary key (root_id, id),

	time_created timestamp not null default now(),
	user_created name not null default current_user,
	time_updated timestamp not null default now(),
	user_updated name not null default current_user,

	ttl_absolute interval,
	deadline_absolute timestamp,
	ttl_relative interval,
	deadline_relative timestamp,

	is_faulted boolean not null default false,
	fault_info text,

	state atom_state not null default 'committing',
	compensation_attempts int not null default 0,

	input text,
	context text,
	output text,

	constraint self_root_no_parent_check check ((id = root_id) = (parent_id is null)),
	constraint no_output_without_input_check check (not (output is not null and input is null)),
	constraint no_children_with_input_check check
		(not (active_child_id is not null and input is not null)),

	constraint no_absolute_deadline_if_not_committing_check check
		(not (deadline_absolute is not null and state != 'committing')),
	constraint no_relative_deadline_if_not_committing_check check
		(not (deadline_relative is not null and state != 'committing'))
);

create type atom_log_action as enum (
	'insert',
	'update'
);

create sequence atom_log_id;

create table atom_log (
	log_id int unique,
	log_action atom_log_action not null,

	like atom,

	primary key (root_id, log_id),

	constraint root_id_fk foreign key (root_id) references atom (id) match full on delete cascade
);



private MinimalPerfectHashing() {
}

public static class Data {

    public int[] gs;
    public int[] vs;

    public Data(int[] gs, int[] vs) {
        this.gs = gs;
        this.vs = vs;
    }
}

public static interface Hasher <K> {

    public long hash(int d, K key);
}

// Computes a minimal perfect hash table using the given python dictionary. It
// returns a tuple (G, V). G and V are both arrays. G contains the intermediate
// table of values needed to compute the index of the value in V. V contains the
// values of the dictionary.
// Source: http://stevehanov.ca/blog/index.php?id=119
// TODO(wtimoney): disk-back this.
public static <K> Data create(Map<K, Integer> dct, Hasher<K> hasher) {
    assert dct.size() > 0;

    int size = dct.size();

    // Step 1: Place all of the keys into buckets
    @SuppressWarnings("unchecked")
    List<K>[] buckets = new List[size];
    for (int i = 0; i < size; ++i)
        buckets[i] = new ArrayList<>();
    int[] gs = new int[size];
    Integer[] vs = new Integer[size];

    for (K key : dct.keySet())
        buckets[(int) (hasher.hash(0, key) % size)].add(key);

    // Step 2: Sort the buckets and process the ones with the most items first.
    Arrays.sort(buckets, new Comparator<List<K>>() {
        @Override
        public int compare(List<K> strings, List<K> strings2) {
            return strings2.size() - strings.size();
        }
    });

    int b = 0;
    for (; b < size; ++b) {
        List<K> bucket = buckets[b];
        if (bucket.size() <= 1)
            break;

        int d = 1;
        int item = 0;
        List<Integer> slots = new ArrayList<>();

        // Repeatedly try different values of d until we find a hash function
        // that places all items in the bucket into free slots
        while (item < bucket.size()) {
            int slot = (int) (hasher.hash(d, bucket.get(item)) % size);

            if (vs[slot] != null || slots.contains(slot)) {
                d += 1;
                item = 0;
                slots = new ArrayList<>();
            } else {
                slots.add(slot);
                item += 1;
            }
        }

        gs[(int) (hasher.hash(0, bucket.get(0)) % size)] = d;

        for (int i = 0; i < bucket.size(); ++i)
            vs[slots.get(i)] = dct.get(bucket.get(i));
    }

    // Only buckets with 1 item remain. Process them more quickly by directly
    // placing them into a free slot. Use a negative value of d to indicate
    // this.
    List<Integer> freelist = new ArrayList<>();
    for (int i = 0; i < size; ++i)
        if (vs[i] == null)
            freelist.add(i);

    for (; b < size; ++b) {
        List<K> bucket = buckets[b];
        if (bucket.size() == 0)
            break;

        int slot = freelist.remove(freelist.size() - 1);

        // We subtract one to ensure it's negative even if the zeroeth slot was
        // used.
        gs[(int) (hasher.hash(0, bucket.get(0)) % size)] = -slot - 1;

        vs[slot] = dct.get(bucket.get(0));
    }

    int[] vsa = new int[vs.length];
    for (int i = 0; i < vs.length; ++i)
        vsa[i] = vs[i];

    return new Data(gs, vsa);
}

// Look up a value in the hash table, defined by G and V.
public static <K> int lookup(int[] gs, int[] vs, K key, Hasher<K> hasher) {
    int d = gs[(int) (hasher.hash(0, key) % gs.length)];

    if (d < 0)
        return vs[-d - 1];

    return vs[(int) (hasher.hash(d, key) % vs.length)];
}

public static <K> void verify(int[] gs, int[] vs, Map<K, Integer> dct, Hasher<K> hasher) {
    for (Map.Entry<K, Integer> e : dct.entrySet()) {
        K k = e.getKey();
        int v = e.getValue();
        int v_ = lookup(gs, vs, k, hasher);

        if (v != v_)
            throw new IllegalStateException(); // ValueError((k, v, v_))
    }
}

private static final long FNV_32_KEY = 0x01000193L;

// Calculates a distinct hash function for a given string. Each value of the
// integer d results in a different hash value.
public static long fnv32Hash(long d, byte[] b) {
    if (d == 0)
        d = FNV_32_KEY;

    // Use the FNV algorithm from http://isthe.com/chongo/tech/comp/fnv/
    for (int i = 0; i < b.length; ++i) {
        byte c = b[i];
        d = ((d * FNV_32_KEY) ^ c) & 0xffffffffL;
    }

    return d;
}

public static Hasher<String> FNV_32_STRING_HASHER = new Hasher<String>() {
    @Override
    public long hash(int d, String key) {
        return fnv32Hash(d, key.getBytes());
    }
};



memcached cachedump


http://stackoverflow.com/a/2946402 > http://docs.oracle.com/javase/8/docs/api/javax/tools/JavaCompiler.html

msgpack
bson

schema inference :/
