auto jdbc partitioning
further jdbc predicate pushdown
 - composite pk scrolling :|
jdbc aggregate pushdown
yarn
 - http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/WritingYarnApplications.html
(python) ffi
event sources
random access stores
first layer caching
vitess shit
nashorn stored procs
salesforce connector?
hbase plz

local fs / hive configurator

https://www.simba.com/data-access/apache-drill-data-sources-file-types
SELECT name1[‘nestedname1’][‘nestednestedname1’] FROM <schema>.<filename>.json

tachyon temp spilling
sorted [leveldb] join storage

schema tracker interface
carbide's full ddl parser impl
service impl

yaml config + velocity templating

BLOOM FILTER PREDICATE PUSHDOWN
 - naw temp tables - engine=memory, index(), pk
 - no, TEMP TABLES :D engine=memory, pk clustered + indices

move kafka decoder to base?
 - no, allow cross-plugin dep, recursive plugin loading
  - dag?

necessary for chunked operation:
bounded in-clause pushdown
occasional join pushdown
view inlining

connector capability enumeration
 - crud
 - complex tuple domains
  - joins, bitwise
  - nested disjuncts?

whitebox / blackbox plugins?


rbr tailing
integrate carbide ddl parser to directly modify metadata (WOW)


nested types? subtables? disjoint? untyped?


collection aggregates (windows? subselects? typed?)


presto decoder = generic connector wrapper, given a bytes col or single col
 - support for disjoints ala dot-separated column names
 - col regexes? select r(x.*)?
 - schema validation
 - + ffi = http plugin?

wrmsr-packaging? bootstrap?
 - full shading
 - JAVA_HOME

jython
 - a fucking sqlalchemy connector why not

mvel
java
jsr223 artifact resolver?
bulk funcs

http://qubole-eng.quora.com/Caching-in-Presto


** make wrapper hierarchical poms like hadoop does, one per shaded subdep

https://github.com/apache/hadoop-common/tree/trunk/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell

https://github.com/shyiko/mysql-binlog-connector-java
https://github.com/addthis/stream-lib

http://blog.sonatype.com/2008/04/how-to-share-resources-across-projects-in-maven/

<dependency>
    <groupId>org.rocksdb</groupId>
    <artifactId>rocksdbjni</artifactId>
    <version>3.10.1</version>
</dependency>

REDSHIFT

eek need on-board views

https://github.com/rcongiu/Hive-JSON-Serde
https://github.com/FasterXML/jackson-dataformat-avro
https://github.com/FasterXML/jackson-datatype-hppc
https://github.com/FasterXML/jackson-dataformat-yaml

https://github.com/facebook/presto/pull/2896

https://cwiki.apache.org/confluence/display/Hive/Tutorial
https://cwiki.apache.org/confluence/display/Hive/Home


* dumb flat json file output
 - complicated by clustering, can just dump to dirs with uuid names as write-only


prepared statements / plan caching



constants?
 - can impl cheap via fns that just return a #, can transform to literals via MH lookup or iface introspection or something

https://github.com/wrmsr/presto-streaming omfg


bitwise operators

hierarchical config - can just fucking redundantly serialize to fit into map<str,str>'s



http://jyni.org/

streams + sqs + gearman

https://github.com/johnewart/gearman-java




lucene analysis scalars


https://github.com/elastic/elasticsearch-hadoop
https://github.com/elastic/stream2es
https://github.com/elastic/elasticsearch-aws


es bulk by length
https://github.com/elastic/elasticsearch-hadoop/tree/master/mr/src/main/java/org/elasticsearch/hadoop
https://github.com/elastic/elasticsearch-hadoop/tree/master/hive/src/main/java/org/elasticsearch/hadoop/hive
https://github.com/elastic/elasticsearch-hadoop/tree/master/yarn



https://github.com/searchbox-io/Jest/tree/master/jest


https://developer.salesforce.com/page/Streaming_API
http://www.salesforce.com/us/developer/docs/api_rest/index_Left.htm#StartTopic=Content/quickstart.htm


parameterized views [indexer generators]



https://mariadb.com/kb/en/mariadb/optimization-and-tuning/
http://phoenix.apache.org/





https://github.com/facebook/presto/pull/2896

https://commons.apache.org/proper/commons-configuration/userguide/howto_utilities.html




https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-Built-inTable-GeneratingFunctions(UDTF)


wrapper will dedupe unshaded jars, write nested classloader
- http://one-jar.sourceforge.net/

rsync powered jarsync bash/py script pls


decoder predicate pushdown to capable stores lols


split on scalar fn

00000000: 2321 2f62 696e 2f73 680a 0a65 7865 6320  #!/bin/sh..exec
00000010: 6a61 7661 202d 586d 7831 4720 2d6a 6172  java -Xmx1G -jar
00000020: 2022 2430 2220 2224 4022 0a0a 504b 0304   "$0" "$@"..PK..



    @Override
    public CompletableFuture<List<ConnectorSplit>> getNextBatch(int maxSize)
    {
        return targets.getNextBatch(maxSize).thenApply(l -> l.stream().map(this::split).collect(Collectors.toList()));
    }


** CHUNK JDBC RETRIEVAL
multi-queries? (dependent partitions)


custom mysql / postgres jdbc? wrmsr-mysql / wrmsr/-postgres?
 - for more efficient multi-sessioning?
  - ideally keep jdbc agnostic

json extraction to unnest

scalar jackson serdes, ideally strongly typeable


src / sink for line files, one text column
 - ffi page sink, post directly to somewhere?

scalar calc parallelization (for cpu-heavy ffi's)
 - special case of batch scalar execution?


http://dev.mysql.com/doc/connector-j/en/connector-j-usagenotes-j2ee-concepts-managing-load-balanced-connections.html

https://github.com/facebook/mysql-5.6/commit/f8e361952612d00979f7cf744f487e48b15cb5a6#diff-1b7266575f084a759d5bee343efe91d0
http://www.oracle.com/technetwork/articles/database-performance/geist-parallel-execution-1-1872400.html


flat:
 - \n, \0, fixedwidth
 - filename regex, string formatted filename

TODO:
 - decoder meta vs unnested extractors :| perf?
 - layered connectors of same schemas as fallbacks (memcache -> mysql)
 - attempt oracle jdbc driver loading
  - fuck that its not in central lmfao eat shit
 - indexer & partitioner metas

ffi to arbitrary jar / classname / methodname
 - + src / sinks


reactor
yarn
 - mesos?

um, shit: struct / union datatypes - only need to be write-only, can fallback to json
 - ... RowParametricType?

select struct('name', name', 'date', date) from ...
 - just make user defined structs :|

view struct inference / auto gen
 - session-specific typesystems?


spark-style stupid jdbc baked query sources
spark-style cache wrapper (for like json files)


redshift:
 - http://docs.aws.amazon.com/redshift/latest/dg/c_redshift_system_overview.html
 - will DEF need (math) agg pushdown lols
 - http://docs.aws.amazon.com/redshift/latest/mgmt/configuring-connections.html#connecting-drivers
 - http://docs.aws.amazon.com/redshift/latest/mgmt/configure-jdbc-connection.html wtf also not in fucking central jfc
  - at least you dont have to fucking sign in
  https://s3.amazonaws.com/redshift-downloads/drivers/RedshiftJDBC41-1.1.1.0001.jar

https://github.com/liquibase/liquibase/tree/master/liquibase-core/src/main/java/liquibase

https://orainternals.files.wordpress.com/2008/07/riyaj_redo_internals_and_tuning_by_redo_reduction_doc.pdf
http://docs.oracle.com/cd/B28359_01/server.111/b28322/gen_rep.htm#STREP011

union connector
 - step 1 just so you dont have to keep typing the fucking catalog.schema. prefix
 - step 2 optinoally combine tables in both if they have same schema
  - naw thats just a fucking union view


hardcoded:
 - views
  - types
 - tables



public class CachingConnectorMetadata
    implements ConnectorMetadata

temporary connector equiv to hardcoded - views + tables
 - naw just an h2 factory that auto-creates a temp schema

generate views upon request - is this macros?



cfg'd in-mem | master thrift port'd hive metastore

autoexec's


http://stackoverflow.com/questions/10929369/how-to-execute-multiple-sql-statements-from-java

try to get a hierarchical subconfig, then try to get a str

memcache is kV only, see what Cass does

dense key ranges only

unpickling?

handlersocket + all that intermediate agg shit

https://github.com/dropbox/PyHive

user defined types via ROW types (w/ named fields)
 struct_extract(field_name, obj) -> val
 auto-gen structs for services based on schemas
 https://github.com/swagger-api/swagger-codegen
 https://github.com/swagger-api/swagger-spec/blob/master/versions/2.0.md

cfg:
 properties
 json
 xml
 yaml

decode:
 raw
 struct
 json
 csv
 xml
 yaml
 avro
 csv/tsv/piped
 cbor
 smile
 pickle

lines
datetimes
deep

http://basho.com/why-vector-clocks-are-hard/
http://basho.com/why-vector-clocks-are-easy/
http://phoenix.apache.org/update_statistics.html
http://en.wikipedia.org/wiki/Behavior_Trees
http://www.cs.man.ac.uk/~norm/papers/surveys.pdf

http://game.itu.dk/cig2010/proceedings/papers/cig10_015_075.pdf


exception handling for bad data :|
 - shit that wont decompress, bad json, etc


            ArchiveInputStream input = new ArchiveStreamFactory()
                    .createArchiveInputStream(originalInput);


record-level connector cache for configs in sqlite and shit
 - convert to scalar for hash lookups?
  - .... just cache the hash join guts?

CAN REPRESENT TYPE-LEVEL CONSTS AS DATALESS TYPES
 compressed_varbinary<bzip>
 could possibly represent gzipped file of json lines as a type, unnest for rows
 jackson<json> -> some_struct
 peanos lols



need eager wrapper on lazy defaults for compression streams

https://prestodb.io/docs/current/connector/kafka-tutorial.html
https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-core-types.html



https://github.com/FasterXML/jackson-module-swagger
 - needs fork + update

set type?
 - just a Map<K, Void>
  - ... so Void type
   - subclassable for typelevel constants


<dependency>
    <groupId>com.jcraft</groupId>
    <artifactId>jsch</artifactId>
    <version>0.1.53</version>
</dependency>

<dependency>
    <groupId>net.razorvine</groupId>
    <artifactId>pyrolite</artifactId>
    <version>4.6</version>
</dependency>



https://github.com/RGBz/aws-s3-class-loader
https://github.com/sampov2/onejar-maven-plugin


<dependency>
    <groupId>org.rocksdb</groupId>
    <artifactId>rocksdbjni</artifactId>
    <version>3.9.1</version>
</dependency>


    // setFetchSize(Integer.MIN_VALUE) is a mysql driver specific way to force streaming results,
    // rather than pulling entire resultset into memory.
    // see http://dev.mysql.com/doc/refman/5.0/en/connector-j-reference-implementation-notes.html
    if (conn.getMetaData.getURL.matches("jdbc:mysql:.*")) {
      stmt.setFetchSize(Integer.MIN_VALUE)
      logInfo("statement fetch size set to: " + stmt.getFetchSize + " to force MySQL streaming ")
    }


types expose funcs w same name that return new instances
execute hardcoded jdbc connectors with where 0=1 if cols not specified
multijvm support to bypass heap size limits



***
just implement a flat raptor StorageEngine, wrap with encoder


http://stackoverflow.com/questions/15524995/adding-another-projects-jar-as-a-resource-using-maven


immediate priorities: codecs, raw raptor storage, spilling, join ordering, bitwise scalar funcs (or, and, xor)


http://www.adellera.it/blog/category/materialized-views/
http://www.adellera.it/blog/2013/04/22/fast-refresh-of-outer-join-only-materialized-views-algorithm-part-1/


HLists? HNode + HNil?

gson?
 http://www.doublecloud.org/2015/03/gson-vs-jackson-which-to-use-for-json-in-java/


https://github.com/facebook/presto/pull/3016 :D
https://github.com/facebook/presto/pull/1937
https://github.com/facebook/presto/pull/3021


<dependency>
    <groupId>redis.clients</groupId>
    <artifactId>jedis</artifactId>
    <version>2.7.2</version>
</dependency>

<dependency>
    <groupId>net.spy</groupId>
    <artifactId>spymemcached</artifactId>
    <version>2.12.0</version>
</dependency>

<dependency>
    <groupId>org.ow2.sat4j</groupId>
    <artifactId>org.ow2.sat4j.core</artifactId>
    <version>2.3.5</version>
</dependency>


https://github.com/yesmeck/jquery-jsonview


com.facebook.presto.operator.PagesIndex <--- DISK SPILL


** NEED TO PUT FAT ROWS LAST. (review text)


on rows:
 - pack fixed
 - pack many into blocks of N - compressssssion
  - just a special case of array/map?
   - row_array? :p

http://www.datastax.com/dev/blog/advanced-time-series-with-cassandra
http://www.tpc.org/tpc_documents_current_versions/pdf/tpch2.17.1.pdf
riak crdt's
https://github.com/FasterXML/jackson-module-swagger
https://github.com/swagger-api/swagger-core/blob/master/pom.xml
http://mesos.apache.org/api/latest/java/
http://docs.oracle.com/cd/B19306_01/server.102/b14220/transact.htm
http://docs.oracle.com/cd/B28359_01/server.111/b28310/onlineredo001.htm#ADMIN11302
http://ss64.com/ora/syntax-redo.html
http://www.pgcon.org/2012/schedule/attachments/258_212_Internals%20Of%20PostgreSQL%20Wal.pdf
https://hackage.haskell.org/package/HList
https://wiki.haskell.org/Heterogenous_collections
http://okmij.org/ftp/Haskell/HList-ext.pdf
http://www.csee.umbc.edu/courses/undergraduate/202/spring07/Lectures/ChangSynopses/modules/m25-hlist/HList.cpp
https://en.wikipedia.org/wiki/Java_bytecode_instruction_listings
https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-4.html
https://www.elastic.co/blog/es-hadoop-2-1-rc1-released

user defined type aliases? parser type literal limitation bypass?

http://mesos.apache.org/api/latest/java/org/apache/mesos/Scheduler.html
https://github.com/apache/mesos/blob/master/src/examples/java/TestFramework.java


find java 8 shebang, exec with minimal heap, spawn child w flock and die

create type atom_state as enum (
	'committing',
	'committed',
	'compensating',
	'compensated'
);

create sequence atom_id;

create table atom (
	id int unique not null,

	root_id int not null references atom (id) on delete restrict,
	parent_id int references atom (id) on delete restrict,
	prev_sibling_id int references atom (id) on delete restrict,
	active_child_id int references atom (id) on delete restrict,

	primary key (root_id, id),

	time_created timestamp not null default now(),
	user_created name not null default current_user,
	time_updated timestamp not null default now(),
	user_updated name not null default current_user,

	ttl_absolute interval,
	deadline_absolute timestamp,
	ttl_relative interval,
	deadline_relative timestamp,

	is_faulted boolean not null default false,
	fault_info text,

	state atom_state not null default 'committing',
	compensation_attempts int not null default 0,

	input text,
	context text,
	output text,

	constraint self_root_no_parent_check check ((id = root_id) = (parent_id is null)),
	constraint no_output_without_input_check check (not (output is not null and input is null)),
	constraint no_children_with_input_check check
		(not (active_child_id is not null and input is not null)),

	constraint no_absolute_deadline_if_not_committing_check check
		(not (deadline_absolute is not null and state != 'committing')),
	constraint no_relative_deadline_if_not_committing_check check
		(not (deadline_relative is not null and state != 'committing'))
);

create type atom_log_action as enum (
	'insert',
	'update'
);

create sequence atom_log_id;

create table atom_log (
	log_id int unique,
	log_action atom_log_action not null,

	like atom,

	primary key (root_id, log_id),

	constraint root_id_fk foreign key (root_id) references atom (id) match full on delete cascade
);



private MinimalPerfectHashing() {
}

public static class Data {

    public int[] gs;
    public int[] vs;

    public Data(int[] gs, int[] vs) {
        this.gs = gs;
        this.vs = vs;
    }
}

public static interface Hasher <K> {

    public long hash(int d, K key);
}

// Computes a minimal perfect hash table using the given python dictionary. It
// returns a tuple (G, V). G and V are both arrays. G contains the intermediate
// table of values needed to compute the index of the value in V. V contains the
// values of the dictionary.
// Source: http://stevehanov.ca/blog/index.php?id=119
// TODO(wtimoney): disk-back this.
public static <K> Data create(Map<K, Integer> dct, Hasher<K> hasher) {
    assert dct.size() > 0;

    int size = dct.size();

    // Step 1: Place all of the keys into buckets
    @SuppressWarnings("unchecked")
    List<K>[] buckets = new List[size];
    for (int i = 0; i < size; ++i)
        buckets[i] = new ArrayList<>();
    int[] gs = new int[size];
    Integer[] vs = new Integer[size];

    for (K key : dct.keySet())
        buckets[(int) (hasher.hash(0, key) % size)].add(key);

    // Step 2: Sort the buckets and process the ones with the most items first.
    Arrays.sort(buckets, new Comparator<List<K>>() {
        @Override
        public int compare(List<K> strings, List<K> strings2) {
            return strings2.size() - strings.size();
        }
    });

    int b = 0;
    for (; b < size; ++b) {
        List<K> bucket = buckets[b];
        if (bucket.size() <= 1)
            break;

        int d = 1;
        int item = 0;
        List<Integer> slots = new ArrayList<>();

        // Repeatedly try different values of d until we find a hash function
        // that places all items in the bucket into free slots
        while (item < bucket.size()) {
            int slot = (int) (hasher.hash(d, bucket.get(item)) % size);

            if (vs[slot] != null || slots.contains(slot)) {
                d += 1;
                item = 0;
                slots = new ArrayList<>();
            } else {
                slots.add(slot);
                item += 1;
            }
        }

        gs[(int) (hasher.hash(0, bucket.get(0)) % size)] = d;

        for (int i = 0; i < bucket.size(); ++i)
            vs[slots.get(i)] = dct.get(bucket.get(i));
    }

    // Only buckets with 1 item remain. Process them more quickly by directly
    // placing them into a free slot. Use a negative value of d to indicate
    // this.
    List<Integer> freelist = new ArrayList<>();
    for (int i = 0; i < size; ++i)
        if (vs[i] == null)
            freelist.add(i);

    for (; b < size; ++b) {
        List<K> bucket = buckets[b];
        if (bucket.size() == 0)
            break;

        int slot = freelist.remove(freelist.size() - 1);

        // We subtract one to ensure it's negative even if the zeroeth slot was
        // used.
        gs[(int) (hasher.hash(0, bucket.get(0)) % size)] = -slot - 1;

        vs[slot] = dct.get(bucket.get(0));
    }

    int[] vsa = new int[vs.length];
    for (int i = 0; i < vs.length; ++i)
        vsa[i] = vs[i];

    return new Data(gs, vsa);
}

// Look up a value in the hash table, defined by G and V.
public static <K> int lookup(int[] gs, int[] vs, K key, Hasher<K> hasher) {
    int d = gs[(int) (hasher.hash(0, key) % gs.length)];

    if (d < 0)
        return vs[-d - 1];

    return vs[(int) (hasher.hash(d, key) % vs.length)];
}

public static <K> void verify(int[] gs, int[] vs, Map<K, Integer> dct, Hasher<K> hasher) {
    for (Map.Entry<K, Integer> e : dct.entrySet()) {
        K k = e.getKey();
        int v = e.getValue();
        int v_ = lookup(gs, vs, k, hasher);

        if (v != v_)
            throw new IllegalStateException(); // ValueError((k, v, v_))
    }
}

private static final long FNV_32_KEY = 0x01000193L;

// Calculates a distinct hash function for a given string. Each value of the
// integer d results in a different hash value.
public static long fnv32Hash(long d, byte[] b) {
    if (d == 0)
        d = FNV_32_KEY;

    // Use the FNV algorithm from http://isthe.com/chongo/tech/comp/fnv/
    for (int i = 0; i < b.length; ++i) {
        byte c = b[i];
        d = ((d * FNV_32_KEY) ^ c) & 0xffffffffL;
    }

    return d;
}

public static Hasher<String> FNV_32_STRING_HASHER = new Hasher<String>() {
    @Override
    public long hash(int d, String key) {
        return fnv32Hash(d, key.getBytes());
    }
};



memcached cachedump


http://stackoverflow.com/a/2946402 > http://docs.oracle.com/javase/8/docs/api/javax/tools/JavaCompiler.html

msgpack
bson

schema inference :/


chronicle !

serialize, serialize_raw


https://en.wikipedia.org/wiki/EAR_(file_format)


raptor > https://github.com/mpetazzoni/ttorrent
https://github.com/JorenSix/TarsosLSH well hello.




olol fml

[INFO] Including org.apache.commons:commons-lang3:jar:3.4 in the shaded jar.
[WARNING] log4j-1.2.17.jar, log4j-over-slf4j-1.7.12.jar define 29 overlappping classes:
[WARNING]   - org.apache.log4j.spi.OptionHandler
[WARNING]   - org.apache.log4j.spi.LoggerRepository
[WARNING]   - org.apache.log4j.NDC
[WARNING]   - org.apache.log4j.PatternLayout
[WARNING]   - org.apache.log4j.LogManager
[WARNING]   - org.apache.log4j.BasicConfigurator
[WARNING]   - org.apache.log4j.spi.ErrorHandler
[WARNING]   - org.apache.log4j.MDC
[WARNING]   - org.apache.log4j.AppenderSkeleton
[WARNING]   - org.apache.log4j.WriterAppender
[WARNING]   - 19 more...
[WARNING] jcl-over-slf4j-1.7.12.jar, commons-logging-1.1.3.jar define 6 overlappping classes:
[WARNING]   - org.apache.commons.logging.impl.SimpleLog$1
[WARNING]   - org.apache.commons.logging.Log
[WARNING]   - org.apache.commons.logging.impl.SimpleLog
[WARNING]   - org.apache.commons.logging.LogConfigurationException
[WARNING]   - org.apache.commons.logging.impl.NoOpLog
[WARNING]   - org.apache.commons.logging.LogFactory
[WARNING] commons-collections-3.2.1.jar, commons-beanutils-1.7.0.jar, commons-beanutils-core-1.8.0.jar define 10 overlappping classes:
[WARNING]   - org.apache.commons.collections.FastHashMap$EntrySet
[WARNING]   - org.apache.commons.collections.FastHashMap$KeySet
[WARNING]   - org.apache.commons.collections.FastHashMap$CollectionView$CollectionViewIterator
[WARNING]   - org.apache.commons.collections.ArrayStack
[WARNING]   - org.apache.commons.collections.FastHashMap$Values
[WARNING]   - org.apache.commons.collections.FastHashMap$CollectionView
[WARNING]   - org.apache.commons.collections.FastHashMap$1
[WARNING]   - org.apache.commons.collections.Buffer
[WARNING]   - org.apache.commons.collections.FastHashMap
[WARNING]   - org.apache.commons.collections.BufferUnderflowException
[WARNING] slf4j-jdk14-1.7.12.jar, slf4j-log4j12-1.7.10.jar define 3 overlappping classes:
[WARNING]   - org.slf4j.impl.StaticMarkerBinder
[WARNING]   - org.slf4j.impl.StaticLoggerBinder
[WARNING]   - org.slf4j.impl.StaticMDCBinder
[WARNING] hadoop-yarn-common-2.7.0.jar, hadoop-yarn-client-2.7.0.jar define 2 overlappping classes:
[WARNING]   - org.apache.hadoop.yarn.client.api.package-info
[WARNING]   - org.apache.hadoop.yarn.client.api.impl.package-info
[WARNING] hadoop-yarn-common-2.7.0.jar, hadoop-yarn-api-2.7.0.jar define 3 overlappping classes:
[WARNING]   - org.apache.hadoop.yarn.util.package-info
[WARNING]   - org.apache.hadoop.yarn.factories.package-info
[WARNING]   - org.apache.hadoop.yarn.factory.providers.package-info


Downloaded: https://oss.sonatype.org/content/repositories/snapshots/com/facebook/presto/presto-server/0.109-SNAPSHOT/presto-server-0.109-20150625.205553-7.jar (626 B at 2.9 KB/sec)
[INFO]
[INFO] --- maven-dependency-plugin:2.10:tree (default-cli) @ presto-wrmsr-wrapper ---
[INFO] com.wrmsr.presto:presto-wrmsr-wrapper:jar:0.109-SNAPSHOT
[INFO] +- com.google.guava:guava:jar:18.0:compile
[INFO] +- io.airlift.resolver:resolver:jar:1.1:compile
[INFO] |  +- org.sonatype.aether:aether-spi:jar:1.13.1:compile
[INFO] |  +- org.sonatype.aether:aether-api:jar:1.13.1:compile
[INFO] |  +- org.sonatype.aether:aether-impl:jar:1.13.1:compile
[INFO] |  +- org.sonatype.aether:aether-util:jar:1.13.1:compile
[INFO] |  +- org.sonatype.aether:aether-connector-file:jar:1.13.1:compile
[INFO] |  +- org.sonatype.aether:aether-connector-asynchttpclient:jar:1.13.1:compile
[INFO] |  |  \- com.ning:async-http-client:jar:1.6.5:compile
[INFO] |  +- io.netty:netty:jar:3.7.0.Final:runtime
[INFO] |  +- org.apache.maven:maven-core:jar:3.0.4:compile
[INFO] |  |  +- org.apache.maven:maven-settings:jar:3.0.4:compile
[INFO] |  |  +- org.apache.maven:maven-settings-builder:jar:3.0.4:compile
[INFO] |  |  +- org.apache.maven:maven-repository-metadata:jar:3.0.4:compile
[INFO] |  |  +- org.apache.maven:maven-plugin-api:jar:3.0.4:compile
[INFO] |  |  +- org.apache.maven:maven-model-builder:jar:3.0.4:compile
[INFO] |  |  +- org.codehaus.plexus:plexus-interpolation:jar:1.14:compile
[INFO] |  |  +- org.codehaus.plexus:plexus-utils:jar:2.0.6:compile
[INFO] |  |  +- org.codehaus.plexus:plexus-component-annotations:jar:1.5.5:compile
[INFO] |  |  \- org.sonatype.plexus:plexus-sec-dispatcher:jar:1.3:compile
[INFO] |  +- org.apache.maven:maven-model:jar:3.0.4:compile
[INFO] |  +- org.apache.maven:maven-artifact:jar:3.0.4:compile
[INFO] |  +- org.apache.maven:maven-aether-provider:jar:3.0.4:compile
[INFO] |  +- org.apache.maven:maven-embedder:jar:3.0.4:runtime
[INFO] |  |  +- org.apache.maven:maven-compat:jar:3.0.4:runtime
[INFO] |  |  |  \- org.apache.maven.wagon:wagon-provider-api:jar:2.2:runtime
[INFO] |  |  \- org.sonatype.plexus:plexus-cipher:jar:1.7:compile
[INFO] |  +- org.codehaus.plexus:plexus-container-default:jar:1.5.5:compile
[INFO] |  |  \- org.apache.xbean:xbean-reflect:jar:3.4:compile
[INFO] |  +- org.codehaus.plexus:plexus-classworlds:jar:2.4:compile
[INFO] |  +- com.google.inject:guice:jar:4.0-beta5:compile
[INFO] |  |  \- aopalliance:aopalliance:jar:1.0:compile
[INFO] |  \- org.slf4j:slf4j-api:jar:1.7.12:compile
[INFO] +- io.airlift:log:jar:0.111:compile
[INFO] |  \- javax.inject:javax.inject:jar:1:compile
[INFO] +- io.airlift:log-manager:jar:0.111:compile
[INFO] |  +- io.airlift:configuration:jar:0.111:compile
[INFO] |  |  +- javax.validation:validation-api:jar:1.1.0.Final:compile
[INFO] |  |  +- org.apache.bval:bval-jsr303:jar:0.5:compile
[INFO] |  |  |  +- org.apache.bval:bval-core:jar:0.5:compile
[INFO] |  |  |  \- org.apache.commons:commons-lang3:jar:3.1:compile
[INFO] |  |  +- com.google.inject.extensions:guice-multibindings:jar:4.0-beta5:compile
[INFO] |  |  \- cglib:cglib-nodep:jar:2.2.2:compile
[INFO] |  +- org.slf4j:slf4j-jdk14:jar:1.7.12:runtime
[INFO] |  +- org.slf4j:log4j-over-slf4j:jar:1.7.12:runtime
[INFO] |  +- org.slf4j:jcl-over-slf4j:jar:1.7.12:runtime
[INFO] |  +- ch.qos.logback:logback-core:jar:1.0.13:compile
[INFO] |  +- org.weakref:jmxutils:jar:1.18:compile
[INFO] |  \- com.google.code.findbugs:annotations:jar:2.0.3:compile
[INFO] +- org.apache.mesos:mesos:jar:0.22.1:compile
[INFO] |  \- com.google.protobuf:protobuf-java:jar:2.4.1:compile
[INFO] +- org.apache.hadoop:hadoop-yarn-client:jar:2.7.0:compile
[INFO] |  +- commons-logging:commons-logging:jar:1.1.3:compile
[INFO] |  +- commons-lang:commons-lang:jar:2.6:compile
[INFO] |  +- commons-cli:commons-cli:jar:1.2:compile
[INFO] |  +- log4j:log4j:jar:1.2.17:compile
[INFO] |  \- org.apache.hadoop:hadoop-annotations:jar:2.7.0:compile
[INFO] |     \- jdk.tools:jdk.tools:jar:1.8:system
[INFO] +- org.apache.hadoop:hadoop-common:jar:2.7.0:compile
[INFO] |  +- org.apache.commons:commons-math3:jar:3.2:compile
[INFO] |  +- xmlenc:xmlenc:jar:0.52:compile
[INFO] |  +- commons-httpclient:commons-httpclient:jar:3.1:compile
[INFO] |  +- commons-codec:commons-codec:jar:1.9:compile
[INFO] |  +- commons-io:commons-io:jar:2.4:compile
[INFO] |  +- commons-net:commons-net:jar:3.1:compile
[INFO] |  +- commons-collections:commons-collections:jar:3.2.1:compile
[INFO] |  +- javax.servlet:servlet-api:jar:2.5:compile
[INFO] |  +- org.mortbay.jetty:jetty:jar:6.1.26:compile
[INFO] |  +- org.mortbay.jetty:jetty-util:jar:6.1.26:compile
[INFO] |  +- javax.servlet.jsp:jsp-api:jar:2.1:runtime
[INFO] |  +- com.sun.jersey:jersey-core:jar:1.9:compile
[INFO] |  +- com.sun.jersey:jersey-json:jar:1.9:compile
[INFO] |  |  \- com.sun.xml.bind:jaxb-impl:jar:2.2.3-1:compile
[INFO] |  +- com.sun.jersey:jersey-server:jar:1.9:compile
[INFO] |  |  \- asm:asm:jar:3.1:compile
[INFO] |  +- net.java.dev.jets3t:jets3t:jar:0.9.0:compile
[INFO] |  |  +- org.apache.httpcomponents:httpclient:jar:4.1.2:compile
[INFO] |  |  +- org.apache.httpcomponents:httpcore:jar:4.1.2:compile
[INFO] |  |  \- com.jamesmurty.utils:java-xmlbuilder:jar:0.4:compile
[INFO] |  +- commons-configuration:commons-configuration:jar:1.6:compile
[INFO] |  |  +- commons-digester:commons-digester:jar:1.8:compile
[INFO] |  |  |  \- commons-beanutils:commons-beanutils:jar:1.7.0:compile
[INFO] |  |  \- commons-beanutils:commons-beanutils-core:jar:1.8.0:compile
[INFO] |  +- org.slf4j:slf4j-log4j12:jar:1.7.10:runtime
[INFO] |  +- org.codehaus.jackson:jackson-core-asl:jar:1.9.13:compile
[INFO] |  +- org.codehaus.jackson:jackson-mapper-asl:jar:1.9.13:compile
[INFO] |  +- org.apache.avro:avro:jar:1.7.4:compile
[INFO] |  |  +- com.thoughtworks.paranamer:paranamer:jar:2.3:compile
[INFO] |  |  \- org.xerial.snappy:snappy-java:jar:1.1.1.3:compile
[INFO] |  +- com.google.code.gson:gson:jar:2.2.4:compile
[INFO] |  +- org.apache.hadoop:hadoop-auth:jar:2.7.0:compile
[INFO] |  |  +- org.apache.directory.server:apacheds-kerberos-codec:jar:2.0.0-M15:compile
[INFO] |  |  |  +- org.apache.directory.server:apacheds-i18n:jar:2.0.0-M15:compile
[INFO] |  |  |  +- org.apache.directory.api:api-asn1-api:jar:1.0.0-M20:compile
[INFO] |  |  |  \- org.apache.directory.api:api-util:jar:1.0.0-M20:compile
[INFO] |  |  \- org.apache.curator:curator-framework:jar:2.7.1:compile
[INFO] |  +- com.jcraft:jsch:jar:0.1.42:compile
[INFO] |  +- org.apache.curator:curator-client:jar:2.7.1:compile
[INFO] |  +- org.apache.curator:curator-recipes:jar:2.7.1:compile
[INFO] |  +- com.google.code.findbugs:jsr305:jar:3.0.0:compile
[INFO] |  +- org.apache.htrace:htrace-core:jar:3.1.0-incubating:compile
[INFO] |  +- org.apache.zookeeper:zookeeper:jar:3.3.6:compile
[INFO] |  \- org.apache.commons:commons-compress:jar:1.4.1:compile
[INFO] |     \- org.tukaani:xz:jar:1.0:compile
[INFO] +- org.apache.hadoop:hadoop-yarn-api:jar:2.7.0:compile
[INFO] +- org.apache.hadoop:hadoop-yarn-common:jar:2.7.0:compile
[INFO] |  +- javax.xml.bind:jaxb-api:jar:2.2.2:compile
[INFO] |  |  +- javax.xml.stream:stax-api:jar:1.0-2:compile
[INFO] |  |  \- javax.activation:activation:jar:1.1:compile
[INFO] |  +- com.sun.jersey:jersey-client:jar:1.9:compile
[INFO] |  +- org.codehaus.jackson:jackson-jaxrs:jar:1.9.13:compile
[INFO] |  +- org.codehaus.jackson:jackson-xc:jar:1.9.13:compile
[INFO] |  +- com.google.inject.extensions:guice-servlet:jar:4.0-beta5:compile
[INFO] |  \- com.sun.jersey.contribs:jersey-guice:jar:1.9:compile
[INFO] +- com.github.jnr:jnr-ffi:jar:2.0.3:compile
[INFO] |  +- com.github.jnr:jffi:jar:1.2.9:compile
[INFO] |  +- com.github.jnr:jffi:jar:native:1.2.9:runtime
[INFO] |  +- org.ow2.asm:asm:jar:5.0.3:compile
[INFO] |  +- org.ow2.asm:asm-commons:jar:5.0.3:compile
[INFO] |  +- org.ow2.asm:asm-analysis:jar:5.0.3:compile
[INFO] |  +- org.ow2.asm:asm-tree:jar:5.0.3:compile
[INFO] |  +- org.ow2.asm:asm-util:jar:5.0.3:compile
[INFO] |  \- com.github.jnr:jnr-x86asm:jar:1.0.2:compile
[INFO] +- com.github.jnr:jnr-posix:jar:3.0.14:compile
[INFO] |  \- com.github.jnr:jnr-constants:jar:0.8.8:compile
[INFO] +- org.testng:testng:jar:6.8.7:test
[INFO] |  +- org.beanshell:bsh:jar:2.0b4:test
[INFO] |  +- com.beust:jcommander:jar:1.27:test
[INFO] |  \- org.yaml:snakeyaml:jar:1.12:test
[INFO] +- junit:junit:jar:4.11:test
[INFO] |  \- org.hamcrest:hamcrest-core:jar:1.3:test
[INFO] +- org.apache.hadoop:hadoop-yarn-server-nodemanager:jar:2.7.0:test
[INFO] |  +- org.codehaus.jettison:jettison:jar:1.1:compile
[INFO] |  +- org.apache.hadoop:hadoop-yarn-server-common:jar:2.7.0:test
[INFO] |  \- org.fusesource.leveldbjni:leveldbjni-all:jar:1.8:test
[INFO] +- org.apache.hadoop:hadoop-yarn-server-resourcemanager:jar:2.7.0:test
[INFO] |  +- org.apache.hadoop:hadoop-yarn-server-applicationhistoryservice:jar:2.7.0:test
[INFO] |  +- org.apache.hadoop:hadoop-yarn-server-web-proxy:jar:2.7.0:test
[INFO] |  \- org.apache.zookeeper:zookeeper:test-jar:tests:3.4.6:test
[INFO] +- org.apache.hadoop:hadoop-yarn-server-tests:test-jar:tests:2.7.0:test
[INFO] \- com.facebook.presto:presto-server:jar:0.109-SNAPSHOT:test



**** represent tables themselves as scalar values to be unnested? gzip<table<json<lucy_document>>> ....
 - subclass of array


** switch to autoexec-ing void sql functions style for cfg setup
defstruct cmd, connect cmd, ...



<dependency>
    <groupId>org.apache.derby</groupId>
    <artifactId>derby</artifactId>
    <version>10.11.1.1</version>
</dependency>

https://cwiki.apache.org/confluence/display/Hive/HiveDerbyServerMode
https://svn.apache.org/repos/asf/hive/branches/branch-0.6/conf/hive-default.xml
http://www.cloudera.com/content/cloudera/en/documentation/archives/cdh3/v3u6/CDH3-Installation-Guide/cdh3ig_topic_16_3.html
http://www.cloudera.com/content/cloudera/en/documentation/cdh4/v4-2-0/CDH4-Installation-Guide/cdh4ig_topic_18_4.html
https://www.elastic.co/blog/elasticsearch-for-apache-hadoop-2-1-spark-storm-and-more
