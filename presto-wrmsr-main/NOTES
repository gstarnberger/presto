http://www.wiscorp.com/sql_2003_standard.zip


com.facebook.presto.operator.PagesIndex

TODO do what LauncherModule does if only to expose presto module instrumentation


***
 zipkin integration via TraceTokenModule - through to subordinate cpy's

https://github.com/klbostee/feathers/blob/master/src/output/RawFileOutputFormat.java

insert into blah.my_s3."/some/path-%d.json" select *

could just hijack org.apache.hadoop.fs.FileSystem
 + LocalFileSystem



java src ffi < - yes do this for youknowwhat
jar native ffi (+ cfgable classpath)
.. native native ffi via jnr/jffi
https://www.py4j.org/
https://blogs.oracle.com/sundararajan/entry/nashorn_javascript_access_to_python
lololol pickled funcs like ibis
ibis -> "unified columnar interchange" -> batched sharedmem ffi
http://www.slideshare.net/wesm/ibis-scaling-python-analytics-on-hadoop-and-impala


graphql connector binding:



jackson-jq
also look at sql2003 xml shit

https://github.com/linkedin/gobblin

https://github.com/apache/bookkeeper


swagger -> https://github.com/square/javapoet
package io.airlift.http.client;
https://github.com/Yelp/bravado

 log-config-file: logback.xml
 log-config: |
   <xml>
   ...
   </xml>

class ReactorsConfig extends MapConfig<String, ReactorConfig>

class ReactorConfig {
    ...
    String viewName;
     - xor -
    String statement;
    List<String> notifyEmails; ***
}

^^ tweaks and flags and shit here, like optionally storing last-seen images
 - on that note short circuit whereever possible
 - manual stage buffer / delay settings
  - ideally adaptive at runtime by default


class ReactorState {
    String name; ?
    String initialJarHead;
    String lastJarHead;
    ...
    Map<String, Position> positions;
    PlanNode originalPlan;
    RecctorConfig initialConfig;
    ReactorConfig lastConfig;
}


*** on graceful shutdown / failure
 - adhoc: tear down intermediates, dead letter delivery
 - daemon: commit, state storage


http://www.tpc.org/tpc_documents_current_versions/pdf/tpch2.17.1.pdf


<dependency>
    <groupId>com.facebook.jcommon</groupId>
    <artifactId>stats</artifactId>
    <version>0.1.23</version>
</dependency>

periodic status emails :D
 - just fuckin do this via a log w an SMTPAppender
also performance monitoring / stats - profiling, counts per step, intermediate index size / churn, etc
can store stats in state


healthcheck endpoint, reactor/exec aware



may well need commit log:
 - shit mode = just cass blob
 - prob kafka sync
 - bookeeper?
  - https://blog.twitter.com/2014/manhattan-our-real-time-multi-tenant-distributed-database-for-twitter-scale
 - !! or can i get away with inline undo logs per record? replay from last checkpoint apply undos when <
  - source merging = ?





pk annotation type
pks via wrapper types -> move to spi
need to store serialized intermediate types -> separate initial and current state rows?




watchdog SUBPROCESS
healthchecks
infinitely sized mr mode?
would be barrier'd multiple queries to s3











auto jdbc partitioning
further jdbc predicate pushdown
 - composite pk scrolling :|
jdbc aggregate pushdown
yarn
 - http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/WritingYarnApplications.html
(python) ffi
event sources
random access stores
first layer caching
vitess shit

nashorn stored procs
salesforce connector?
hbase plz

local fs / hive configurator

https://www.simba.com/data-access/apache-drill-data-sources-file-types
SELECT name1[‘nestedname1’][‘nestednestedname1’] FROM <schema>.<filename>.json

tachyon temp spilling
sorted [leveldb] join storage

schema tracker interface
carbide's full ddl parser impl
service impl

yaml config + velocity templating

BLOOM FILTER PREDICATE PUSHDOWN
 - naw temp tables - engine=memory, index(), pk
 - no, TEMP TABLES :D engine=memory, pk clustered + indices

move kafka decoder to base?
 - no, allow cross-plugin dep, recursive plugin loading
  - dag?

necessary for chunked operation:
bounded in-clause pushdown
occasional join pushdown
view inlining

connector capability enumeration
 - crud
 - complex tuple domains
  - joins, bitwise
  - nested disjuncts?

whitebox / blackbox plugins?


rbr tailing
integrate carbide ddl parser to directly modify metadata (WOW)


nested types? subtables? disjoint? untyped?


collection aggregates (windows? subselects? typed?)


presto decoder = generic connector wrapper, given a bytes col or single col
 - support for disjoints ala dot-separated column names
 - col regexes? select r(x.*)?
 - schema validation
 - + ffi = http plugin?

wrmsr-packaging? bootstrap?
 - full shading
 - JAVA_HOME

jython
 - a fucking sqlalchemy connector why not

mvel
java
jsr223 artifact resolver?
bulk funcs

http://qubole-eng.quora.com/Caching-in-Presto


** make wrapper hierarchical poms like hadoop does, one per shaded subdep

<dependency>
    <groupId>org.rocksdb</groupId>
    <artifactId>rocksdbjni</artifactId>
    <version>3.10.1</version>
</dependency>

REDSHIFT

eek need on-board views


* dumb flat json file output
 - complicated by clustering, can just dump to dirs with uuid names as write-only


prepared statements / plan caching



constants?
 - can impl cheap via fns that just return a #, can transform to literals via MH lookup or iface introspection or something

https://github.com/wrmsr/presto-streaming omfg


bitwise operators
hash algs



http://jyni.org/

streams + sqs + gearman

https://github.com/johnewart/gearman-java




lucene analysis scalars


es bulk by length


parameterized views [indexer generators]



decoder predicate pushdown to capable stores lols


split on scalar fn


    @Override
    public CompletableFuture<List<ConnectorSplit>> getNextBatch(int maxSize)
    {
        return targets.getNextBatch(maxSize).thenApply(l -> l.stream().map(this::split).collect(Collectors.toList()));
    }


** CHUNK JDBC RETRIEVAL
multi-queries? (dependent partitions)


custom mysql / postgres jdbc? wrmsr-mysql / wrmsr/-postgres?
 - for more efficient multi-sessioning?
  - ideally keep jdbc agnostic

json extraction to unnest

scalar jackson serdes, ideally strongly typeable


src / sink for line files, one text column
 - ffi page sink, post directly to somewhere?

scalar calc parallelization (for cpu-heavy ffi's)
 - special case of batch scalar execution?


flat:
 - \n, \0, fixedwidth
 - filename regex, string formatted filename

TODO:
 - decoder meta vs unnested extractors :| perf?
 - layered connectors of same schemas as fallbacks (memcache -> mysql)
 - attempt oracle jdbc driver loading
  - fuck that its not in central lmfao eat shit
 - indexer & partitioner metas

ffi to arbitrary jar / classname / methodname
 - + src / sinks


reactor
yarn
 - mesos?

um, shit: struct / union datatypes - only need to be write-only, can fallback to json
 - ... RowParametricType?

select struct('name', name', 'date', date) from ...
 - just make user defined structs :|

view struct inference / auto gen
 - session-specific typesystems?


spark-style stupid jdbc baked query sources
spark-style cache wrapper (for like json files)


redshift:
 - http://docs.aws.amazon.com/redshift/latest/dg/c_redshift_system_overview.html
 - will DEF need (math) agg pushdown lols
 - http://docs.aws.amazon.com/redshift/latest/mgmt/configuring-connections.html#connecting-drivers
 - http://docs.aws.amazon.com/redshift/latest/mgmt/configure-jdbc-connection.html wtf also not in fucking central jfc
  - at least you dont have to fucking sign in
  https://s3.amazonaws.com/redshift-downloads/drivers/RedshiftJDBC41-1.1.1.0001.jar

https://github.com/liquibase/liquibase/tree/master/liquibase-core/src/main/java/liquibase

https://orainternals.files.wordpress.com/2008/07/riyaj_redo_internals_and_tuning_by_redo_reduction_doc.pdf
http://docs.oracle.com/cd/B28359_01/server.111/b28322/gen_rep.htm#STREP011

union connector
 - step 1 just so you dont have to keep typing the fucking catalog.schema. prefix
 - step 2 optinoally combine tables in both if they have same schema
  - naw thats just a fucking union view


hardcoded:
 - views
  - types
 - tables



public class CachingConnectorMetadata
    implements ConnectorMetadata

temporary connector equiv to hardcoded - views + tables
 - naw just an h2 factory that auto-creates a temp schema

generate views upon request - is this macros?



cfg'd in-mem | master thrift port'd hive metastore

autoexec's



try to get a hierarchical subconfig, then try to get a str

memcache is kV only, see what Cass does

dense key ranges only

unpickling?

handlersocket + all that intermediate agg shit

https://github.com/dropbox/PyHive

user defined types via ROW types (w/ named fields)
 struct_extract(field_name, obj) -> val
 auto-gen structs for services based on schemas
 https://github.com/swagger-api/swagger-codegen
 https://github.com/swagger-api/swagger-spec/blob/master/versions/2.0.md

cfg:
 properties
 json
 xml
 yaml

decode:
 raw
 struct
 json
 csv
 xml
 yaml
 avro
 csv/tsv/piped
 cbor
 smile
 pickle

lines
datetimes
deep


exception handling for bad data :|
 - shit that wont decompress, bad json, etc


            ArchiveInputStream input = new ArchiveStreamFactory()
                    .createArchiveInputStream(originalInput);


record-level connector cache for configs in sqlite and shit
 - convert to scalar for hash lookups?
  - .... just cache the hash join guts?

CAN REPRESENT TYPE-LEVEL CONSTS AS DATALESS TYPES
 compressed_varbinary<bzip>
 could possibly represent gzipped file of json lines as a type, unnest for rows
 jackson<json> -> some_struct
 peanos lols



need eager wrapper on lazy defaults for compression streams




https://github.com/FasterXML/jackson-module-swagger
 - needs fork + update

set type?
 - just a Map<K, Void>
  - ... so Void type
   - subclassable for typelevel constants


<dependency>
    <groupId>net.razorvine</groupId>
    <artifactId>pyrolite</artifactId>
    <version>4.6</version>
</dependency>



    // setFetchSize(Integer.MIN_VALUE) is a mysql driver specific way to force streaming results,
    // rather than pulling entire resultset into memory.
    // see http://dev.mysql.com/doc/refman/5.0/en/connector-j-reference-implementation-notes.html
    if (conn.getMetaData.getURL.matches("jdbc:mysql:.*")) {
      stmt.setFetchSize(Integer.MIN_VALUE)
      logInfo("statement fetch size set to: " + stmt.getFetchSize + " to force MySQL streaming ")
    }


types expose funcs w same name that return new instances
execute hardcoded jdbc connectors with where 0=1 if cols not specified
multijvm support to bypass heap size limits



***
just implement a flat raptor StorageEngine, wrap with encoder




immediate priorities: codecs, raw raptor storage, spilling, join ordering, bitwise scalar funcs (or, and, xor)


http://www.adellera.it/blog/category/materialized-views/
http://www.adellera.it/blog/2013/04/22/fast-refresh-of-outer-join-only-materialized-views-algorithm-part-1/


HLists? HNode + HNil?

gson?


<dependency>
    <groupId>redis.clients</groupId>
    <artifactId>jedis</artifactId>
    <version>2.7.2</version>
</dependency>

<dependency>
    <groupId>net.spy</groupId>
    <artifactId>spymemcached</artifactId>
    <version>2.12.0</version>
</dependency>

<dependency>
    <groupId>org.ow2.sat4j</groupId>
    <artifactId>org.ow2.sat4j.core</artifactId>
    <version>2.3.5</version>
</dependency>


https://github.com/yesmeck/jquery-jsonview


com.facebook.presto.operator.PagesIndex <--- DISK SPILL


** NEED TO PUT FAT ROWS LAST. (review text)


on rows:
 - pack fixed
 - pack many into blocks of N - compressssssion
  - just a special case of array/map?
   - row_array? :p

riak crdt's

user defined type aliases? parser type literal limitation bypass?



find java 8 shebang, exec with minimal heap, spawn child w flock and die

create type atom_state as enum (
    'committing',
    'committed',
    'compensating',
    'compensated'
);

create sequence atom_id;

create table atom (
    id int unique not null,

    root_id int not null references atom (id) on delete restrict,
    parent_id int references atom (id) on delete restrict,
    prev_sibling_id int references atom (id) on delete restrict,
    active_child_id int references atom (id) on delete restrict,

    primary key (root_id, id),

    time_created timestamp not null default now(),
    user_created name not null default current_user,
    time_updated timestamp not null default now(),
    user_updated name not null default current_user,

    ttl_absolute interval,
    deadline_absolute timestamp,
    ttl_relative interval,
    deadline_relative timestamp,

    is_faulted boolean not null default false,
    fault_info text,

    state atom_state not null default 'committing',
    compensation_attempts int not null default 0,

    input text,
    context text,
    output text,

    constraint self_root_no_parent_check check ((id = root_id) = (parent_id is null)),
    constraint no_output_without_input_check check (not (output is not null and input is null)),
    constraint no_children_with_input_check check
        (not (active_child_id is not null and input is not null)),

    constraint no_absolute_deadline_if_not_committing_check check
        (not (deadline_absolute is not null and state != 'committing')),
    constraint no_relative_deadline_if_not_committing_check check
        (not (deadline_relative is not null and state != 'committing'))
);

create type atom_log_action as enum (
    'insert',
    'update'
);

create sequence atom_log_id;

create table atom_log (
    log_id int unique,
    log_action atom_log_action not null,

    like atom,

    primary key (root_id, log_id),

    constraint root_id_fk foreign key (root_id) references atom (id) match full on delete cascade
);



private MinimalPerfectHashing() {
}

public static class Data {

    public int[] gs;
    public int[] vs;

    public Data(int[] gs, int[] vs) {
        this.gs = gs;
        this.vs = vs;
    }
}

public static interface Hasher <K> {

    public long hash(int d, K key);
}

// Computes a minimal perfect hash table using the given python dictionary. It
// returns a tuple (G, V). G and V are both arrays. G contains the intermediate
// table of values needed to compute the index of the value in V. V contains the
// values of the dictionary.
// Source: http://stevehanov.ca/blog/index.php?id=119
// TODO(wtimoney): disk-back this.
public static <K> Data create(Map<K, Integer> dct, Hasher<K> hasher) {
    assert dct.size() > 0;

    int size = dct.size();

    // Step 1: Place all of the keys into buckets
    @SuppressWarnings("unchecked")
    List<K>[] buckets = new List[size];
    for (int i = 0; i < size; ++i)
        buckets[i] = new ArrayList<>();
    int[] gs = new int[size];
    Integer[] vs = new Integer[size];

    for (K key : dct.keySet())
        buckets[(int) (hasher.hash(0, key) % size)].add(key);

    // Step 2: Sort the buckets and process the ones with the most items first.
    Arrays.sort(buckets, new Comparator<List<K>>() {
        @Override
        public int compare(List<K> strings, List<K> strings2) {
            return strings2.size() - strings.size();
        }
    });

    int b = 0;
    for (; b < size; ++b) {
        List<K> bucket = buckets[b];
        if (bucket.size() <= 1)
            break;

        int d = 1;
        int item = 0;
        List<Integer> slots = new ArrayList<>();

        // Repeatedly try different values of d until we find a hash function
        // that places all items in the bucket into free slots
        while (item < bucket.size()) {
            int slot = (int) (hasher.hash(d, bucket.get(item)) % size);

            if (vs[slot] != null || slots.contains(slot)) {
                d += 1;
                item = 0;
                slots = new ArrayList<>();
            } else {
                slots.add(slot);
                item += 1;
            }
        }

        gs[(int) (hasher.hash(0, bucket.get(0)) % size)] = d;

        for (int i = 0; i < bucket.size(); ++i)
            vs[slots.get(i)] = dct.get(bucket.get(i));
    }

    // Only buckets with 1 item remain. Process them more quickly by directly
    // placing them into a free slot. Use a negative value of d to indicate
    // this.
    List<Integer> freelist = new ArrayList<>();
    for (int i = 0; i < size; ++i)
        if (vs[i] == null)
            freelist.add(i);

    for (; b < size; ++b) {
        List<K> bucket = buckets[b];
        if (bucket.size() == 0)
            break;

        int slot = freelist.remove(freelist.size() - 1);

        // We subtract one to ensure it's negative even if the zeroeth slot was
        // used.
        gs[(int) (hasher.hash(0, bucket.get(0)) % size)] = -slot - 1;

        vs[slot] = dct.get(bucket.get(0));
    }

    int[] vsa = new int[vs.length];
    for (int i = 0; i < vs.length; ++i)
        vsa[i] = vs[i];

    return new Data(gs, vsa);
}

// Look up a value in the hash table, defined by G and V.
public static <K> int lookup(int[] gs, int[] vs, K key, Hasher<K> hasher) {
    int d = gs[(int) (hasher.hash(0, key) % gs.length)];

    if (d < 0)
        return vs[-d - 1];

    return vs[(int) (hasher.hash(d, key) % vs.length)];
}

public static <K> void verify(int[] gs, int[] vs, Map<K, Integer> dct, Hasher<K> hasher) {
    for (Map.Entry<K, Integer> e : dct.entrySet()) {
        K k = e.getKey();
        int v = e.getValue();
        int v_ = lookup(gs, vs, k, hasher);

        if (v != v_)
            throw new IllegalStateException(); // ValueError((k, v, v_))
    }
}

private static final long FNV_32_KEY = 0x01000193L;

// Calculates a distinct hash function for a given string. Each value of the
// integer d results in a different hash value.
public static long fnv32Hash(long d, byte[] b) {
    if (d == 0)
        d = FNV_32_KEY;

    // Use the FNV algorithm from http://isthe.com/chongo/tech/comp/fnv/
    for (int i = 0; i < b.length; ++i) {
        byte c = b[i];
        d = ((d * FNV_32_KEY) ^ c) & 0xffffffffL;
    }

    return d;
}

public static Hasher<String> FNV_32_STRING_HASHER = new Hasher<String>() {
    @Override
    public long hash(int d, String key) {
        return fnv32Hash(d, key.getBytes());
    }
};



memcached cachedump



msgpack
bson

schema inference :/


chronicle !

serialize, serialize_raw




raptor > https://github.com/mpetazzoni/ttorrent
https://github.com/JorenSix/TarsosLSH well hello.

**** represent tables themselves as scalar values to be unnested? gzip<table<json<lucy_document>>> ....
 - subclass of array


** switch to autoexec-ing void sql functions style for cfg setup
defstruct cmd, connect cmd, ...


<dependency>
    <groupId>org.apache.derby</groupId>
    <artifactId>derby</artifactId>
    <version>10.11.1.1</version>
</dependency>

start stop run status mesos yarn cli

find jdk dl link lel
restart kill
docker aware, find oracle j8 Dockerfile

wrapper config aware, yelpify dockerism in there



gon need curator / zk for state storage



via http://mesos.apache.org/documentation/latest/docker-containerizer/
 > attach jar and cfg as commandinfo files, override entrypoint to point to jar :D
jarsync self-update and restart endpoint for cluster, jardiff own jar
may also be nice to build docker images w/ deps in ~/.m2/repository, more idiomatic at least
 > but requires private repository


batched update mode? equiv to lazy de/re-jsoning done by ei partial aggs?
 - pulsed/stepped at a regular interval? pipelined at a depth of max joins? (which = execution pipeline itself)

hash join operator -> cassandra
 - or rather kv:x

favor imperative config, include / exec / eval files / text from disk / sql sources
 - lacking loops should get cfg via jsr223 up

async / background query execution ala bash & / fg, both interactive in cli and scriptable

for paasta can just run as adhoc in context of another service, node disco can go through zk
oo and how about a raw mode powered by just jsch, aws disco aware



<dependency>
    <groupId>org.apache.curator</groupId>
    <artifactId>apache-curator</artifactId>
    <version>2.8.0</version>
</dependency>
<dependency>
    <groupId>org.apache.curator</groupId>
    <artifactId>curator-framework</artifactId>
    <version>2.8.0</version>
</dependency>
<dependency>
    <groupId>org.apache.curator</groupId>
    <artifactId>curator-recipes</artifactId>
    <version>2.8.0</version>
</dependency>


http://storagemojo.com/2015/06/29/the-storage-tipping-point/
http://jmoiron.net/blog/thoughts-on-timeseries-databases/

cache invalidator probes for mat-view engine ala cacheserv

jar url plugin loading (for shades)

just install jdk in shebang lols
cfg cmds must run on ALL NODES, make sure cfg does that
make sure node dont take queries till cfg'd

fuck it raw lucene connector



postgres brin

acidy materialization by honoring gtid alignment of events and propagating to final store only when at a txn boundary
 .. where possible ofc

biject structs and schemas (avro, swagger, ...)
pojos/beans for structs for ffi


https://github.com/jenkinsci/ssh-plugin/blob/master/pom.xml#L32 well hellooooo


package org.apache.hadoop.hive.ql.processors;
CommandProcessorFactory

package org.apache.hadoop.hive.metastore;
public class HiveMetaStore extends ThriftHiveMetastore {
  public static void main(String[] args) throws Throwable {

select hive_exec('catalog_name', 'create table ....');


so, uh, limit propagation...

make it ssh to grab fresh s3 keys lols

*** codegen for imperative retrieval of opted-out or non-reactive tables

table aliases, imperative / service tables, DAG for view validation, ...

JGIT INTEGRATION FOR MANAGING REPOS OF INCLUDES

write throttling for daytime - could just be for final store not intermediate store


omg bloom filter / bitvector generator aggregate, exportable for use in py
 - partially updating by events :D
https://issues.jenkins-ci.org/browse/JENKINS-20276



http://www.sqlstyle.guide/

<dependency>
    <groupId>org.functionaljava</groupId>
    <artifactId>functionaljava</artifactId>
    <version>4.4</version>
</dependency>
<dependency>
    <groupId>org.functionaljava</groupId>
    <artifactId>functionaljava-java8</artifactId>
    <version>4.4</version>
</dependency>

:p

https://github.com/aol/cyclops
hm.


with/without provided preimage
postimage can be retrieved albeit leakily


create table out.business (id int primary key, name varchar) as
select id, name from in.business;

business.insert > insert into out.business (id, name) values (new.id, new.name);
business.update > update out.business set out.business.id = new.id, out.business.name = new.name where out.business.id = old.business.id;
business.delete > delete from out.business where out.business.id = old.id;




create table out.review (id int primary key, comment varchar, business_id int, business_name varchar) as
select in.review.id, in.review.comment, in.business.id, in.business.name
from in.review
inner join in.business on in.business.id = in.review.business_id;

???

business.insert > insert into out.business (id, name) values (new.id, new.name);
business.update > update out.business set out.business.id = new.id, out.business.name = new.name where out.business.id = old.business.id;
business.delete > delete from out.business where out.business.id = old.id;



CRUD by pk



es connector







user defined enums?





base jdbc commit batch size hardcoded 1k ruh roh




Is this another case where the application is re-writing a zip file that is in use? This can be worked around by running with the system property sun.zip.disableMemoryMapping set to true.


rlimit: STACK 8192k, CORE 0k, NPROC 709, NOFILE 10240, AS infinity
https://stackoverflow.com/questions/19447444/fatal-errors-from-openjdk-when-running-fresh-jar-files


wget -c --no-cookies --no-check-certificate --header "Cookie: gpw_e24=http%3A%2F%2Fwww.oracle.com%2F; oraclelicense=accept-securebackup-cookie" "https://download.oracle.com/otn-pub/java/jdk/8u60-b27/jdk-8u60-linux-x64.tar.gz" --output-document="jdk-8u60-linux-x64.tar.gz"

8u60-b27

ehehehe


deploy via instance tags



stupid simple fs connector for interactive shit, select name from s3 where path = '/some/dir/*'

** when reexecing set proc title with args, unavailable to ps at launch




instance tags: Name, creator (username), owner (team)




https://zookeeper.apache.org/doc/trunk/zookeeperReconfig.html

https://aws.amazon.com/blogs/aws/new-ec2-spot-instance-termination-notices/

ec2-launch ec2-run, upload to master, distribute from master, clone jdk detection in java

find or dl and install jdk, optional force upgrade
sync jar
launch, master or slave as appropriate

oh shit yeah ec2-profiles, don't need to retype


^in cfg yaml ofc.


non-spot master, spot slaves....


https://software.intel.com/en-us/articles/intel-performance-counter-monitor ehehehee



aws emr create-cluster --name "Test cluster" --ami-version 3.3 \
--applications Name=Hue Name=Hive Name=Pig Name=HBase \
--use-default-roles --ec2-attributes KeyName=myKey \
--instance-type c1.xlarge --instance-count 3 --termination-protected

http://169.254.169.254/latest/user-data

http://docs.datastax.com/en/cassandra/2.2/cassandra/install/installAMILaunch.html

com.mysql.jdbc.ReplicationDriver

.aws/credentials


create external table my_stuff (id string, prev_id string, uri string, extra string) partitioned by (dt string)
 row format serde 'com.amazon.elasticmapreduce.JsonSerde' with serdeproperties ('paths'='id, prev_id, url, extra');
alter table my_ranger add partition (dt='small') location 's3://yelp-emr-dev/foo/';

https://github.com/apache/hive/blob/ac755ebe26361a4647d53db2a28500f71697b276/ql/src/java/org/apache/hadoop/hive/ql/udf/UDFParseUrl.java


https://github.com/confluentinc/bottledwater-pg yay

https://github.com/mpetazzoni/ttorrent/blob/master/cli/src/main/java/com/turn/ttorrent/cli/TorrentMain.java


IT ISNT THE TABLES THAT ARE COMPRESSED IT IS THE FILES. SPLITS OR PARTITIONS.
in that sense yes it is an unnest of rows from n files
no omfg not COMPRESSED<bzip, ... ---- bzip<lines<json<business>>>



  archive)
    CLASS=org.apache.hadoop.tools.HadoopArchives
    hadoop_debug "Injecting TOOL_PATH into CLASSPATH"
    hadoop_add_classpath "${TOOL_PATH}"
  ;;

    CLASS=org.apache.hadoop.fs.FsShell

class S3(BasicCommand):
    NAME = 's3'
    DESCRIPTION = BasicCommand.FROM_FILE('s3/_concepts.rst')
    SYNOPSIS = "aws s3 <Command> [<Arg> ...]"
    SUBCOMMANDS = [
        {'name': 'ls', 'command_class': ListCommand},
        {'name': 'website', 'command_class': WebsiteCommand},
        {'name': 'cp', 'command_class': CpCommand},
        {'name': 'mv', 'command_class': MvCommand},
        {'name': 'rm', 'command_class': RmCommand},
        {'name': 'sync', 'command_class': SyncCommand},
        {'name': 'mb', 'command_class': MbCommand},
        {'name': 'rb', 'command_class': RbCommand}
    ]




aws: profiles: {}
hadoop: ...
mesos: ...
wrapper: clusters: ... [ip's, master, ...]


new cluster cmd w ssh, scp, etc
Cluster cfg, host cfg, some kind of default + override sys
Host, ssh port, node ports, is master, username, PW or ident,
ec2 + emr + uh mrj compute pool clusters?
hehhh hdfs cluster setup + admin
raptor diag
could actually leave hdfs subprocesses running between updates, would need its own pidfile doe
config file merging - order of ops for autoexec? maybe hocon has something relevant? multiple --- yaml docs?
assess shared machine vs dedicated / virtualized machine, adj heap accordingly, overridable ofc
spun up cluster keygen on master ala spark

ssh subprocess fallback

default to presto.yaml | presto.json if not given a -c

cmds: hive hdfs ec2 s3
? yarn emr

sigbus is probs repo deleteOnExit firing before finalizers or some shit run or something, death pact cleaner bash child proc would probs do it
 - will need to open each file individually, just listen on stdin and open shit
 bash -c 'while IFS= read -r FILE; do exec {FD}<"$FILE" 2>/dev/null; done'

uh graphql? wat




<dependency>
    <groupId>org.nd4j</groupId>
    <artifactId>nd4j-jblas</artifactId>
    <version>0.0.3.5.5.4-SNAPSHOT</version>
</dependency>

https://jclouds.apache.org/
 - LOL NOPE GUICE3




un fixme dont fucking provided dep launcher from hadoop and mesos and aws holy fuck look at dem deps

todo its still fucking pathetic to bundle all of hive just for metastore maint... jackson -> thrift pls :[

    <dependency>
        <groupId>net.snaq</groupId>
        <artifactId>dbpool</artifactId>
        <version>7.0-jdk7</version>
    </dependency>

-Dsun.net.spi.nameservice.provider.1=dns,local


FIXME -server lols


    public static final String IPv4_SETTING = "java.net.preferIPv4Stack";
    public static final String IPv6_SETTING = "java.net.preferIPv6Addresses";

https://github.com/elastic/elasticsearch/blob/master/core/src/main/java/org/elasticsearch/common/network/NetworkUtils.java


mosh --ssh="ssh -i wtimoney_dev.pem" ec2-user@


export TERM=screen
echo 'export TERM=screen' >> ~/.bashrc
sudo bash -c 'echo 127.0.0.1 `hostname` >> /etc/hosts'
sudo yum update -y
wget -c --no-cookies --no-check-certificate --header "Cookie: gpw_e24=http%3A%2F%2Fwww.oracle.com%2F; oraclelicense=accept-securebackup-cookie" "https://download.oracle.com/otn-pub/java/jdk/8u60-b27/jdk-8u60-linux-x64.tar.gz" --output-document="jdk-8u60-linux-x64.tar.gz"
tar xvzf jdk-8u60-linux-x64.tar.gz
sudo yum install -y htop tcpdump iotop tmux mtr mosh
wget http://pkgs.repoforge.org/iftop/iftop-0.17-1.el6.rf.x86_64.rpm
sudo rpm -ivh iftop-0.17-1.el6.rf.x86_64.rpm
pip install --user ipython

gcc gcc-c++ :|


~/jdk1.8.0_60/bin/java -Xmx32G -XX:+UseConcMarkSweepGC -server -jar ./presto/presto run

http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-ec2-config.html

c4.8xl = 100 MB/s

m4.10xl = 100 MB/s
m4.4xl = 105 MB/s
m4.2xl = 60 MB/s
m4.xl = 28 MB/s ?

m3.2xl = 46 MB/s
m3.m = shit

i2.8xl = 100 MB/s

r3.8xl = 100 MB/s

d2.8xl = 70 MB/s (cross region)




m4's + c4's ebs-only, future = i2's + d2's only non-ebs - options:
 - spin up mix of i's for storage and c's / m's for compute
  - possibly wastes potential bw
 - run all i's
 - just go straight ramdisk
  - maybe tachyon > hdfs here? puniverse galaxy?
  - uh, would it be better to just make raptor do this natively to Unsafe malloc'd mem?
   - yes.
   - does raptor do replication for datasets significantly smaller than cluster ram?


**** setting a file to read-only will compact it, after which ByteBuffers are untranslated
 - use slices obv
 - make RegularFile take a blockSize override
 - since dealing with variable size blocks just use malloc, huge page alignment is irrelevant given no disk
 - mlock ofc



just fucking embed antlr fuck it

https://software.intel.com/en-us/protected-download/328018/326559/step2
-std=c++0x


http://www.brendangregg.com/blog/2014-09-15/the-msrs-of-ec2.html


http://yusufonlinux.blogspot.com/2010/11/data-link-access-and-zero-copy.html

include/linux/dma-mapping.h
Documentation/DMA-API.txt

.... jcache
 - loooooooool

The big difference between ORC and DWRF is DWRF has a encoding called “row-group dictionary”.  This adds a second dictionary to a column that only applies to the current row group (10k rows).  This can reduce the compressed size of a column when the column has too many unique values for a global dictionary, but not so many unique values that a dictionary is useless.  There are some places where this is a big win.  The downside is complexity in the encoder and decoder.



create table dst as select id pk, txt from src;

on insert
 insert into dst values after.id, after.txt;
 conflict = warn? die? configurable?

on update
 pk resolution
 ... updates...
 generate pre/post img, recurse.......

on delete
 delete from dst where dst.id = before.id;
 missing = same

fan in / fan out // aggregate / join

INDEPENDENT STAGES
exchange?
existing machinery wo last trier ordering?




ok fuck jcache - 1.0.0 v 0.5 v cache-ri-common fuck off get your shit together


        <dependency>
            <groupId>javax.transaction</groupId>
            <artifactId>jta</artifactId>
            <version>1.1</version>
        </dependency>
        <dependency>
            <groupId>javax.cache</groupId>
            <artifactId>cache-api</artifactId>
            <version>0.5</version>
        </dependency>

ughhhh


jenkins Java/JNR ssh-agent
xlat piles to pure j via props
oh yeah cli tabular presentation




sudo apt-get build-dep linux-image-$(uname -r)
apt-get source linux-image-$(uname -r)
cd linux-$(uname -r)

fakeroot debian/rules clean
fakeroot debian/rules binary-headers binary-generic






configurable structural equality for structs?
 define_struct('thing', false, ... ... ...
 define_struct_for_view? type view_struct('view.name')


decode
decode_full
encoded<encoded<business>('json')>('gzip')

diodes lol

select jdbc('conn', 'do raw jdbc stuff') pls


https://github.com/dockerfile/java/tree/master/oracle-java8
jdk.nashorn.tools.Shell


seriously, sigils?
 Nullable<>
 SizeEstimated<>('40M')
 ...
 how does this shit work with math and funcs?
 annotations?




https://github.com/FasterXML/jackson-module-jsonSchema
https://github.com/joelittlejohn/jsonschema2pojo
http://swagger.io/specification/#schemaObject
https://github.com/everit-org/json-schema/tree/master/core


reinterpret_cast
 as_gzip?

ffi > scripting

http://techblog.netflix.com/2015/02/rad-outlier-detection-on-big-data.html?m=1 ayy grrl

local fig
uuid, master








christ.
Merge
 Replace vs Update
 deep vs shallow
 default values
 Optional<>



TEACH ABOUT TMP DIRS. UGH.


properties['log.levels-file'] = options.log_levels
properties['log.output-file'] = options.server_log
properties['log.enable-console'] = 'false'







HiveSplitManager.getPartitions()



TODO: 2-layer repositories - dump / read shared to ~/.m2, dump blacklisted (all presto-related) to tmp



apache oryx tejo kylin
ok so use asg's i guess



ConnectorKv, RxBatchedKv
 - its all in wrmsr-main so muh

Example 5 — Creating a routine mapping
CREATE ROUTINE MAPPING FN1_AT_FS1
 FOR SCH.FUN1(VARCHAR, INTEGER)
 SERVER FS1
 OPTIONS (REMOTE_NAME 'FN1',
 REMOTE_SCHEMA 'TEST')
Example 6 illustrates the use of a mapped function:
Example 6 — Using routine mappings
SELECT resume
FROM EMP
WHERE fun1(name, city_id) = 100;



** kv broadcast to replicated clusters (2 az's)
